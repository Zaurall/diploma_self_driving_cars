{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba045145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45386a5-f7ed-4d02-a0ce-e1b18e2e0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(data, cutoff=2.0, fs=20.0, order=2):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "\n",
    "def fit_scalers(file_paths, sample_size=5000):\n",
    "    sample_paths = random.sample(file_paths, min(len(file_paths), sample_size))\n",
    "    dfs = [pd.read_csv(p)[SAVE_COLUMNS].rename(columns=RENAME_COLUMNS) for p in sample_paths]\n",
    "    # dfs = process_map(pd.read_csv, files, max_workers=24, chunksize=100)\n",
    "    df = pd.concat(dfs) # , ignore_index=True)\n",
    "\n",
    "    scalers = {\n",
    "        'roll': StandardScaler().fit(df[['roll']].values),\n",
    "        'aEgo': StandardScaler().fit(df[['aEgo']].values),\n",
    "        'vEgo': StandardScaler().fit(df[['vEgo']].values),\n",
    "        'targetLateralAcceleration': StandardScaler().fit(df[['targetLateralAcceleration']].values), # RobustScaler() \n",
    "        # 'steerCommand': RobustScaler().fit(df[['steerCommand']].values)\n",
    "    }\n",
    "    return scalers\n",
    "\n",
    "\n",
    "class DrivingDataset(Dataset):\n",
    "    def __init__(self, file_paths, seq_len=20, scalers=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.seq_len = seq_len\n",
    "        self.scalers = scalers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[SAVE_COLUMNS].rename(columns=RENAME_COLUMNS)\n",
    "\n",
    "        if len(df) < self.seq_len + 1:\n",
    "            return self[random.randint(0, len(self) - 1)]  # Skip short episodes\n",
    "\n",
    "        # Apply standard scalers\n",
    "        for col, scaler in self.scalers.items():\n",
    "            df[col] = scaler.transform(df[[col]].values)\n",
    "\n",
    "        # Scale steerCommand using RobustScaler on first 100 rows\n",
    "        first_100 = df.iloc[:100]\n",
    "        steer_scaler = RobustScaler().fit(first_100[['steerCommand']].values)\n",
    "        df['steerCommand'] = steer_scaler.transform(df[['steerCommand']].values)\n",
    "        # df['steerCommand'] = lowpass_filter(df['steerCommand'].values)\n",
    "        # df['steerCommand'] = df['steerCommand'].rolling(5, min_periods=1, center=True).mean()\n",
    "\n",
    "        \n",
    "        # df['steerCommand'] = self.scalers['steerCommand'].transform(df[['steerCommand']].values)\n",
    "\n",
    "\n",
    "        arr = df[['roll', 'aEgo', 'vEgo', 'targetLateralAcceleration', 'steerCommand']].values\n",
    "        physics_input = arr[:self.seq_len, :3]\n",
    "        control_input = arr[:self.seq_len, 3:]\n",
    "        y = arr[1:self.seq_len+1, 4].reshape(-1, 1)  # predict next steerCommand\n",
    "\n",
    "        return (\n",
    "            torch.tensor(physics_input, dtype=torch.float32),\n",
    "            torch.tensor(control_input, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "class LstmEncoderDecoder(nn.Module):\n",
    "    def __init__(self, physics_input_size, control_input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.physics_encoder = nn.LSTM(physics_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.control_encoder = nn.LSTM(control_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(control_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_physics, input_control_sequence):\n",
    "        _, (hidden_phsc, cell_phsc) = self.physics_encoder(input_physics)\n",
    "        _, (hidden_ctrl, cell_ctrl) = self.control_encoder(input_control_sequence)\n",
    "\n",
    "        hidden = (hidden_phsc + hidden_ctrl) / 2\n",
    "        cell = (cell_phsc + cell_ctrl) / 2\n",
    "\n",
    "        decoder_output, _ = self.decoder(input_control_sequence, (hidden, cell))\n",
    "        return self.fc_out(decoder_output)\n",
    "\n",
    "\n",
    "def train_val_split(file_paths, val_ratio=0.2, seed=42):\n",
    "    random.seed(seed)\n",
    "    shuffled = list(file_paths)\n",
    "    random.shuffle(shuffled)\n",
    "    split_idx = int(len(shuffled) * (1 - val_ratio))\n",
    "    return shuffled[:split_idx], shuffled[split_idx:]\n",
    "\n",
    "\n",
    "def train_model(model, model_save_path, train_loader, val_loader, num_epochs=10, lr=1e-3):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [], 'train_mae': [], 'train_rmse': [], 'train_r2': [],\n",
    "        'val_loss': [], 'val_mae': [], 'val_rmse': [], 'val_r2': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_targets = [], []\n",
    "\n",
    "        for physics, control, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
    "            physics, control, y = physics.to(DEVICE), control.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(physics, control)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            all_train_preds.append(pred.detach().cpu().numpy())\n",
    "            all_train_targets.append(y.detach().cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_preds = np.vstack(all_train_preds).reshape(-1)  # Flatten to 1D\n",
    "        train_targets = np.vstack(all_train_targets).reshape(-1)\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_rmse = mean_squared_error(train_targets, train_preds, squared=False)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for physics, control, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
    "                physics, control, y = physics.to(DEVICE), control.to(DEVICE), y.to(DEVICE)\n",
    "                pred = model(physics, control)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                all_val_preds.append(pred.detach().cpu().numpy())\n",
    "                all_val_targets.append(y.detach().cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_preds = np.vstack(all_val_preds).reshape(-1)  # Flatten to 1D\n",
    "        val_targets = np.vstack(all_val_targets).reshape(-1)\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R2: {train_r2:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R2: {val_r2:.4f}\")\n",
    "\n",
    "        # Save history\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_r2'].append(train_r2)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_r2'].append(val_r2)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"{model_save_path}/lstm_best_model.pt\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{model_save_path}/lstm_final.pt\")\n",
    "\n",
    "    # Save history to CSV\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(f\"{model_save_path}/history.csv\", index=False)\n",
    "\n",
    "    # Plot loss and MAE\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['epoch'], history['train_mae'], label='Train MAE')\n",
    "    plt.plot(history['epoch'], history['val_mae'], label='Val MAE')\n",
    "    plt.title('MAE Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_save_path}/loss_and_mae_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98814faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = Path(\"../data/train\")\n",
    "MODEL_VERSION = 'lstm_v3_1000-dataset_lr-3_epochs-20_seq-len-100_local-scaler_steer-filtered_2_256'\n",
    "MODEL_SAVE_PATH = f\"../models/{MODEL_VERSION}\"\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "NUM_WORKERS = 4\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "SAVE_COLUMNS = ['roll', 'aEgo', 'vEgo', 'latAccelSteeringAngle', 'steerFiltered']\n",
    "RENAME_COLUMNS = {\n",
    "    'latAccelSteeringAngle': 'targetLateralAcceleration',\n",
    "    'steerFiltered': 'steerCommand'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c6764-8dc1-4224-b45a-1b8aaef13867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.72it/s]\n",
      "Epoch 1/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 379471.5922, MAE: 4.1199, RMSE: 616.0125, R2: 0.0001 | Val Loss: 72.6342, MAE: 0.4389, RMSE: 8.6238, R2: 0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.81it/s]\n",
      "Epoch 2/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 379374.3573, MAE: 4.0810, RMSE: 615.9336, R2: 0.0003 | Val Loss: 71.0176, MAE: 0.4347, RMSE: 8.5273, R2: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.61it/s]\n",
      "Epoch 3/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 379315.1816, MAE: 4.0790, RMSE: 615.8857, R2: 0.0005 | Val Loss: 69.7682, MAE: 0.4343, RMSE: 8.4519, R2: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.17it/s]\n",
      "Epoch 4/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 379265.8053, MAE: 4.0806, RMSE: 615.8456, R2: 0.0006 | Val Loss: 68.2203, MAE: 0.4325, RMSE: 8.3577, R2: 0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.08it/s]\n",
      "Epoch 5/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 379253.5279, MAE: 4.0771, RMSE: 615.8356, R2: 0.0006 | Val Loss: 67.7658, MAE: 0.4386, RMSE: 8.3298, R2: 0.1193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.88it/s]\n",
      "Epoch 6/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 379203.9040, MAE: 4.0748, RMSE: 615.7952, R2: 0.0008 | Val Loss: 66.5044, MAE: 0.4315, RMSE: 8.2519, R2: 0.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.81it/s]\n",
      "Epoch 7/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 379177.9441, MAE: 4.0745, RMSE: 615.7741, R2: 0.0008 | Val Loss: 65.1055, MAE: 0.4255, RMSE: 8.1646, R2: 0.1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.01it/s]\n",
      "Epoch 8/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 379139.4066, MAE: 4.0732, RMSE: 615.7429, R2: 0.0009 | Val Loss: 64.3018, MAE: 0.4267, RMSE: 8.1141, R2: 0.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.65it/s]\n",
      "Epoch 9/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 379113.8296, MAE: 4.0709, RMSE: 615.7220, R2: 0.0010 | Val Loss: 63.4408, MAE: 0.4253, RMSE: 8.0595, R2: 0.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.05it/s]\n",
      "Epoch 10/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 379070.0498, MAE: 4.0701, RMSE: 615.6866, R2: 0.0011 | Val Loss: 62.7991, MAE: 0.4325, RMSE: 8.0187, R2: 0.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.74it/s]\n",
      "Epoch 11/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 379062.2523, MAE: 4.0699, RMSE: 615.6802, R2: 0.0011 | Val Loss: 62.1951, MAE: 0.4222, RMSE: 7.9800, R2: 0.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.77it/s]\n",
      "Epoch 12/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 379023.2816, MAE: 4.0697, RMSE: 615.6485, R2: 0.0012 | Val Loss: 61.1665, MAE: 0.4238, RMSE: 7.9137, R2: 0.2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.01it/s]\n",
      "Epoch 13/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 378997.9311, MAE: 4.0691, RMSE: 615.6280, R2: 0.0013 | Val Loss: 60.4428, MAE: 0.4208, RMSE: 7.8668, R2: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.88it/s]\n",
      "Epoch 14/20 - Val: 100%|██████████| 32/32 [00:03<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 378955.7464, MAE: 4.0669, RMSE: 615.5939, R2: 0.0014 | Val Loss: 60.0339, MAE: 0.4259, RMSE: 7.8401, R2: 0.2198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train:  18%|█▊        | 22/125 [00:02<00:10, 10.22it/s]"
     ]
    }
   ],
   "source": [
    "all_files = list(DATA_DIR.glob(\"**/*.csv\"))\n",
    "all_files = all_files[:10000]\n",
    "# print(len(all_files))\n",
    "train_files, val_files = train_val_split(all_files, val_ratio=0.2)\n",
    "\n",
    "scalers = fit_scalers(train_files)\n",
    "\n",
    "for s in scalers.values():\n",
    "    if hasattr(s, 'feature_names_in_'):\n",
    "        s.feature_names_in_ = None\n",
    "with open(f\"{MODEL_SAVE_PATH}/scalers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "train_dataset = DrivingDataset(train_files, seq_len=SEQ_LEN, scalers=scalers)\n",
    "val_dataset = DrivingDataset(val_files, seq_len=SEQ_LEN, scalers=scalers)\n",
    "\n",
    "# print(f\"Number of validation files: {len(val_files)}\")\n",
    "# print(f\"Number of samples in validation dataset: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "hidden_size=256\n",
    "num_layers=2\n",
    "\n",
    "model = LstmEncoderDecoder(\n",
    "    physics_input_size=3,\n",
    "    control_input_size=2,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "model = train_model(model, MODEL_SAVE_PATH, train_loader, val_loader,\n",
    "                    num_epochs=NUM_EPOCHS, lr=1e-3)\n",
    "\n",
    "# torch.save(model.state_dict(), f\"{MODEL_SAVE_PATH}/lstm_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de937960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561e085-194f-48fa-8276-1f5272fa6edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
