{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba045145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45386a5-f7ed-4d02-a0ce-e1b18e2e0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(data, cutoff=2.0, fs=20.0, order=2):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "\n",
    "def fit_scalers(file_paths, sample_size=5000):\n",
    "    sample_paths = random.sample(file_paths, min(len(file_paths), sample_size))\n",
    "    dfs = [pd.read_csv(p)[SAVE_COLUMNS].rename(columns=RENAME_COLUMNS) for p in sample_paths]\n",
    "    # dfs = process_map(pd.read_csv, files, max_workers=24, chunksize=100)\n",
    "    df = pd.concat(dfs) # , ignore_index=True)\n",
    "\n",
    "    scalers = {\n",
    "        'roll': StandardScaler().fit(df[['roll']].values),\n",
    "        'aEgo': StandardScaler().fit(df[['aEgo']].values),\n",
    "        'vEgo': StandardScaler().fit(df[['vEgo']].values),\n",
    "        'targetLateralAcceleration': StandardScaler().fit(df[['targetLateralAcceleration']].values), # RobustScaler() \n",
    "        # 'steerCommand': RobustScaler().fit(df[['steerCommand']].values)\n",
    "    }\n",
    "    return scalers\n",
    "\n",
    "\n",
    "class DrivingDataset(Dataset):\n",
    "    def __init__(self, file_paths, seq_len=SEQ_LEN, future_k=FUTURE_K, scalers=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.seq_len = seq_len\n",
    "        self.future_k = future_k\n",
    "        self.scalers = scalers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[SAVE_COLUMNS].rename(columns=RENAME_COLUMNS)\n",
    "\n",
    "        # Need at least seq_len + future horizon + 1 rows\n",
    "        min_needed = self.seq_len + self.future_k\n",
    "        if len(df) < min_needed:\n",
    "            return self[random.randint(0, len(self) - 1)]  # Skip short episodes\n",
    "\n",
    "        # Apply standard scalers\n",
    "        for col, scaler in self.scalers.items():\n",
    "            df[col] = scaler.transform(df[[col]].values)\n",
    "\n",
    "        # Per-file robust scaling for steerCommand using first 100 rows (or entire if shorter)\n",
    "        first_100 = df.iloc[:min(100, len(df))]\n",
    "        steer_scaler = RobustScaler().fit(first_100[['steerCommand']].values)\n",
    "        df['steerCommand'] = steer_scaler.transform(df[['steerCommand']].values)\n",
    "        # df['steerCommand'] = lowpass_filter(df['steerCommand'].values)\n",
    "        # df['steerCommand'] = df['steerCommand'].rolling(5, min_periods=1, center=True).mean()\n",
    "\n",
    "        \n",
    "        # df['steerCommand'] = self.scalers['steerCommand'].transform(df[['steerCommand']].values)\n",
    "\n",
    "\n",
    "        arr = df[['roll', 'aEgo', 'vEgo', 'targetLateralAcceleration', 'steerCommand']].values\n",
    "        physics_input = arr[:self.seq_len, :3]\n",
    "        control_input = arr[:self.seq_len, 3:5]  # targetLatAcc, steerCommand\n",
    "        \n",
    "        # Future plan: first FUTURE_K future target lat acc values immediately after first row\n",
    "        fut_raw = arr[1:1 + self.future_k, 3]  # column index 3 = targetLateralAcceleration\n",
    "        # Pad if needed (should not usually happen due to min_needed) but be safe\n",
    "        if len(fut_raw) < self.future_k:\n",
    "            pad_val = fut_raw[-1] if len(fut_raw) > 0 else 0.0\n",
    "            fut_raw = np.pad(fut_raw, (0, self.future_k - len(fut_raw)), constant_values=pad_val)\n",
    "\n",
    "        # Repeat future plan for each timestep and concatenate to control inputs\n",
    "        future_plan_matrix = np.repeat(fut_raw.reshape(1, -1), self.seq_len, axis=0)  # (seq_len, FUTURE_K)\n",
    "        control_aug = np.concatenate([control_input, future_plan_matrix], axis=1)      # (seq_len, 2 + FUTURE_K)\n",
    "\n",
    "        # Labels: predict next-step steerCommand for each timestep\n",
    "        y = arr[1:self.seq_len + 1, 4].reshape(-1, 1)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(physics_input, dtype=torch.float32),\n",
    "            torch.tensor(control_aug, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "class LstmEncoderDecoder(nn.Module):\n",
    "    def __init__(self, physics_input_size, control_input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.physics_encoder = nn.LSTM(physics_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.control_encoder = nn.LSTM(control_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(control_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_physics, input_control_sequence):\n",
    "        _, (hidden_phsc, cell_phsc) = self.physics_encoder(input_physics)\n",
    "        _, (hidden_ctrl, cell_ctrl) = self.control_encoder(input_control_sequence)\n",
    "\n",
    "        hidden = (hidden_phsc + hidden_ctrl) / 2\n",
    "        cell = (cell_phsc + cell_ctrl) / 2\n",
    "\n",
    "        decoder_output, _ = self.decoder(input_control_sequence, (hidden, cell))\n",
    "        return self.fc_out(decoder_output)\n",
    "\n",
    "\n",
    "def train_val_split(file_paths, val_ratio=0.2, seed=SEED):\n",
    "    random.seed(seed)\n",
    "    shuffled = list(file_paths)\n",
    "    random.shuffle(shuffled)\n",
    "    split_idx = int(len(shuffled) * (1 - val_ratio))\n",
    "    return shuffled[:split_idx], shuffled[split_idx:]\n",
    "\n",
    "\n",
    "def train_model(model, model_save_path, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LR):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    history = defaultdict(list)\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [], 'train_mae': [], 'train_rmse': [], 'train_r2': [],\n",
    "        'val_loss': [], 'val_mae': [], 'val_rmse': [], 'val_r2': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds_all, train_tgts_all = [], []\n",
    "\n",
    "        for physics, control, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
    "            physics, control, y = physics.to(DEVICE), control.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(physics, control)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_preds_all.append(pred.detach().cpu().numpy())\n",
    "            train_tgts_all.append(y.detach().cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_preds = np.vstack(train_preds_all).reshape(-1)  # Flatten to 1D\n",
    "        train_targets = np.vstack(train_tgts_all).reshape(-1)\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_rmse = mean_squared_error(train_targets, train_preds, squared=False)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds_all, val_tgts_all = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for physics, control, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
    "                physics, control, y = physics.to(DEVICE), control.to(DEVICE), y.to(DEVICE)\n",
    "                pred = model(physics, control)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_preds_all.append(pred.detach().cpu().numpy())\n",
    "                val_tgts_all.append(y.detach().cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_preds = np.vstack(val_preds_all).reshape(-1)  # Flatten to 1D\n",
    "        val_targets = np.vstack(val_tgts_all).reshape(-1)\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R2: {train_r2:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R2: {val_r2:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Save history\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_r2'].append(train_r2)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_r2'].append(val_r2)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_save_path / 'lstm_future_best.pt')\n",
    "\n",
    "    torch.save(model.state_dict(), model_save_path / 'lstm_future_final.pt')            \n",
    "\n",
    "    # Save history to CSV\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(f\"{model_save_path}/history.csv\", index=False)\n",
    "\n",
    "    with open(model_save_path / 'history.pkl', 'wb') as f:\n",
    "        pickle.dump(dict(history), f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98814faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = Path(\"../data/train\")\n",
    "\n",
    "LSTM_HIDDEN_SIZE = 256\n",
    "LSTM_NUM_LAYERS = 2\n",
    "SEQ_LEN = 100\n",
    "FUTURE_K = 20        # number of future target lataccel points (Option A)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-3\n",
    "\n",
    "\n",
    "SCALER_TYPE = \"local-scaler\"\n",
    "# SCALER_TYPE = \"global-scaler\"\n",
    "TARGET_COLUMN = \"steer-filtered\"\n",
    "\n",
    "NUMBER_SCENES_IN_DATASET = 10000\n",
    "\n",
    "MODEL_NUMBER_VERSION = 4\n",
    "MODEL_VERSION = f\"v{MODEL_NUMBER_VERSION}_lstm-{LSTM_NUM_LAYERS}-{LSTM_HIDDEN_SIZE}_{NUMBER_SCENES_IN_DATASET}-dataset_lr-{LR}_epochs-{NUM_EPOCHS}_seq-{SEQ_LEN}_{SCALER_TYPE}_futurek-{FUTURE_K}_target-{TARGET_COLUMN}\"\n",
    "MODEL_SAVE_PATH = Path(f\"../models/{MODEL_VERSION}\")\n",
    "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_COLUMNS = ['roll', 'aEgo', 'vEgo', 'latAccelSteeringAngle', 'steerFiltered']\n",
    "RENAME_COLUMNS = {\n",
    "    'latAccelSteeringAngle': 'targetLateralAcceleration',\n",
    "    'steerFiltered': 'steerCommand'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c6764-8dc1-4224-b45a-1b8aaef13867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.72it/s]\n",
      "Epoch 1/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 379471.5922, MAE: 4.1199, RMSE: 616.0125, R2: 0.0001 | Val Loss: 72.6342, MAE: 0.4389, RMSE: 8.6238, R2: 0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.81it/s]\n",
      "Epoch 2/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 379374.3573, MAE: 4.0810, RMSE: 615.9336, R2: 0.0003 | Val Loss: 71.0176, MAE: 0.4347, RMSE: 8.5273, R2: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.61it/s]\n",
      "Epoch 3/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 379315.1816, MAE: 4.0790, RMSE: 615.8857, R2: 0.0005 | Val Loss: 69.7682, MAE: 0.4343, RMSE: 8.4519, R2: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.17it/s]\n",
      "Epoch 4/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 379265.8053, MAE: 4.0806, RMSE: 615.8456, R2: 0.0006 | Val Loss: 68.2203, MAE: 0.4325, RMSE: 8.3577, R2: 0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.08it/s]\n",
      "Epoch 5/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 379253.5279, MAE: 4.0771, RMSE: 615.8356, R2: 0.0006 | Val Loss: 67.7658, MAE: 0.4386, RMSE: 8.3298, R2: 0.1193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.88it/s]\n",
      "Epoch 6/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 379203.9040, MAE: 4.0748, RMSE: 615.7952, R2: 0.0008 | Val Loss: 66.5044, MAE: 0.4315, RMSE: 8.2519, R2: 0.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.81it/s]\n",
      "Epoch 7/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 379177.9441, MAE: 4.0745, RMSE: 615.7741, R2: 0.0008 | Val Loss: 65.1055, MAE: 0.4255, RMSE: 8.1646, R2: 0.1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.01it/s]\n",
      "Epoch 8/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 379139.4066, MAE: 4.0732, RMSE: 615.7429, R2: 0.0009 | Val Loss: 64.3018, MAE: 0.4267, RMSE: 8.1141, R2: 0.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.65it/s]\n",
      "Epoch 9/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 379113.8296, MAE: 4.0709, RMSE: 615.7220, R2: 0.0010 | Val Loss: 63.4408, MAE: 0.4253, RMSE: 8.0595, R2: 0.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.05it/s]\n",
      "Epoch 10/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 379070.0498, MAE: 4.0701, RMSE: 615.6866, R2: 0.0011 | Val Loss: 62.7991, MAE: 0.4325, RMSE: 8.0187, R2: 0.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.74it/s]\n",
      "Epoch 11/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 379062.2523, MAE: 4.0699, RMSE: 615.6802, R2: 0.0011 | Val Loss: 62.1951, MAE: 0.4222, RMSE: 7.9800, R2: 0.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.77it/s]\n",
      "Epoch 12/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 379023.2816, MAE: 4.0697, RMSE: 615.6485, R2: 0.0012 | Val Loss: 61.1665, MAE: 0.4238, RMSE: 7.9137, R2: 0.2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 11.01it/s]\n",
      "Epoch 13/20 - Val: 100%|██████████| 32/32 [00:03<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 378997.9311, MAE: 4.0691, RMSE: 615.6280, R2: 0.0013 | Val Loss: 60.4428, MAE: 0.4208, RMSE: 7.8668, R2: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train: 100%|██████████| 125/125 [00:11<00:00, 10.88it/s]\n",
      "Epoch 14/20 - Val: 100%|██████████| 32/32 [00:03<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 378955.7464, MAE: 4.0669, RMSE: 615.5939, R2: 0.0014 | Val Loss: 60.0339, MAE: 0.4259, RMSE: 7.8401, R2: 0.2198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train:  18%|█▊        | 22/125 [00:02<00:10, 10.22it/s]"
     ]
    }
   ],
   "source": [
    "    all_files = list(DATA_DIR.glob(\"**/*.csv\"))\n",
    "    all_files = all_files[:NUMBER_SCENES_IN_DATASET]\n",
    "    # print(len(all_files))\n",
    "    train_files, val_files = train_val_split(all_files, val_ratio=0.2)\n",
    "    \n",
    "    scalers = fit_scalers(train_files)\n",
    "    \n",
    "    for s in scalers.values():\n",
    "        if hasattr(s, 'feature_names_in_'):\n",
    "            s.feature_names_in_ = None\n",
    "    with open(f\"{MODEL_SAVE_PATH}/scalers.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scalers, f)\n",
    "    \n",
    "    train_dataset = DrivingDataset(train_files, seq_len=SEQ_LEN, scalers=scalers)\n",
    "    val_dataset = DrivingDataset(val_files, seq_len=SEQ_LEN, scalers=scalers)\n",
    "    \n",
    "    # print(f\"Number of validation files: {len(val_files)}\")\n",
    "    # print(f\"Number of samples in validation dataset: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = LstmEncoderDecoder(\n",
    "        physics_input_size=3,\n",
    "        control_input_size=2,\n",
    "        hidden_size=LSTM_HIDDEN_SIZE,\n",
    "        num_layers=LSTM_NUM_LAYERS\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    model = train_model(model, MODEL_SAVE_PATH, train_loader, val_loader,\n",
    "                        num_epochs=NUM_EPOCHS, lr=1e-3)\n",
    "    \n",
    "    # torch.save(model.state_dict(), f\"{MODEL_SAVE_PATH}/lstm_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de937960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561e085-194f-48fa-8276-1f5272fa6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "    # # Plot loss and MAE\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
    "    # plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
    "    # plt.title('Loss Curve')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('MSE Loss')\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(history['epoch'], history['train_mae'], label='Train MAE')\n",
    "    # plt.plot(history['epoch'], history['val_mae'], label='Val MAE')\n",
    "    # plt.title('MAE Curve')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('MAE')\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{model_save_path}/loss_and_mae_plot.png\")\n",
    "    # plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
