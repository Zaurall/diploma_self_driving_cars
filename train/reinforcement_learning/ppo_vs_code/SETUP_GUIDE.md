# PPO Training Setup - Complete Structure

## âœ… Created Directory Structure

```
ppo_training/
â”œâ”€â”€ README.md                    âœ“ Created
â”œâ”€â”€ config.yaml                  âœ“ Created  
â”œâ”€â”€ requirements.txt             âœ“ Created
â”œâ”€â”€ train.py                     âœ“ Created (CLI training script)
â”œâ”€â”€ train.ipynb                  âœ“ Created (Jupyter notebook)
â”œâ”€â”€ eval.py                      âš ï¸ To be created
â”‚
â”œâ”€â”€ environment/                 âœ“ Created
â”‚   â”œâ”€â”€ __init__.py             âœ“
â”‚   â””â”€â”€ lateral_env.py          âœ“ (LateralControlEnv + VectorizedEnv)
â”‚
â”œâ”€â”€ policy/                      âœ“ Created
â”‚   â”œâ”€â”€ __init__.py             âœ“
â”‚   â”œâ”€â”€ actor_critic.py         âœ“ (Actor-Critic network)
â”‚   â””â”€â”€ ppo_agent.py            âœ“ (PPO algorithm)
â”‚
â”œâ”€â”€ utils/                       âœ“ Created
â”‚   â”œâ”€â”€ __init__.py             âœ“
â”‚   â”œâ”€â”€ replay_buffer.py        âœ“ (RolloutBuffer for PPO)
â”‚   â””â”€â”€ logger.py               âœ“ (Tensorboard logger)
â”‚
â”œâ”€â”€ models/                      ğŸ“ (Created during training)
â”‚   â”œâ”€â”€ checkpoint_*.pt         (Training checkpoints)
â”‚   â”œâ”€â”€ final_model.pt          (Final trained model)
â”‚   â””â”€â”€ training_stats.npz      (Training statistics)
â”‚
â””â”€â”€ runs/                        ğŸ“ (Created during training)
    â””â”€â”€ ppo_lateral_control/    (Tensorboard logs)
```

## ğŸš€ Quick Start Guide

### 1. Installation

```bash
cd ppo_training
pip install -r requirements.txt
```

### 2. Training Options

#### Option A: Jupyter Notebook (Recommended for JupyterLab)
```bash
jupyter lab train.ipynb
```
Then run all cells sequentially.

#### Option B: Command Line
```bash
python train.py --config config.yaml
```

### 3. Monitor Training
```bash
tensorboard --logdir=./runs
```

### 4. Evaluation
```bash
python eval.py --model_path models/final_model.pt
```

## ğŸ“Š Key Features

### vs Original AWS Fargate Setup

| Feature | Original | Simplified Version |
|---------|----------|-------------------|
| **Infrastructure** | AWS Fargate (cloud) | Single GPU server (local) |
| **Communication** | GRPC protocol | Direct Python calls |
| **Parallelization** | Multiple containers | Vectorized environments |
| **Scaling** | CI/CD pipeline | Fixed parallel envs |
| **Permissions** | Root required | No root needed |
| **Interface** | CLI only | CLI + Jupyter notebook |

### Advantages of This Setup

âœ… **Single GPU optimized** - Designed for A100  
âœ… **JupyterLab friendly** - Interactive training  
âœ… **No cloud costs** - Runs locally  
âœ… **No root access needed** - User-level installation  
âœ… **Simplified architecture** - Easier to debug  
âœ… **Faster iteration** - No container build times  

## âš™ï¸ Configuration

Edit `config.yaml` to customize:

- **Learning rate**: `3e-4` (default)
- **Num parallel envs**: `8` (adjust based on GPU memory)
- **Batch size**: `256`
- **Total timesteps**: `1,000,000`
- **PPO epochs**: `10`
- **Clip ratio**: `0.2`

## ğŸ“ˆ Expected Results

- Training time: **2-4 hours** on A100
- GPU memory usage: **~6-8 GB**
- Convergence: **~500k-800k timesteps**

## ğŸ”§ Troubleshooting

### Out of Memory (OOM)
- Reduce `num_envs` in config.yaml
- Reduce `batch_size`
- Reduce `hidden_sizes`

### Slow Training
- Increase `num_envs` if memory allows
- Increase `batch_size`
- Use mixed precision training (fp16)

### Poor Performance
- Adjust reward function in `environment/lateral_env.py`
- Tune PPO hyperparameters
- Increase network capacity

## ğŸ“ Next Steps

1. **Run training** - Start with default config
2. **Monitor progress** - Use Tensorboard
3. **Tune hyperparameters** - Based on results
4. **Evaluate** - Test on held-out data
5. **Deploy** - Create controller for inference

## ğŸ¯ Differences from AWS Setup

### Removed Components:
- âŒ Dockerfile
- âŒ GRPC protocol (`rollout.proto`)
- âŒ AWS deployment scripts
- âŒ CI/CD pipeline
- âŒ Client-server architecture

### Added Components:
- âœ… Jupyter notebook interface
- âœ… Simplified training loop
- âœ… Local tensorboard logging
- âœ… Direct environment interaction

## ğŸ“š File Descriptions

### Core Training Files

- **`train.py`** - Main CLI training script with full PPO loop
- **`train.ipynb`** - Interactive Jupyter notebook for training
- **`config.yaml`** - All hyperparameters and settings
- **`eval.py`** - Evaluation script (to be created)

### Environment

- **`lateral_env.py`** - Gym-compatible lateral control environment
  - `LateralControlEnv` - Single environment
  - `VectorizedLateralControlEnv` - Parallel environments

### Policy

- **`actor_critic.py`** - Neural network architecture
  - Shared feature extractor
  - Separate actor (policy) and critic (value) heads
  
- **`ppo_agent.py`** - PPO algorithm implementation
  - GAE computation
  - Policy update with clipping
  - Value function training

### Utils

- **`replay_buffer.py`** - Experience storage for PPO
- **`logger.py`** - Tensorboard logging utilities

## ğŸ’¡ Tips for JupyterLab

1. **Run cells sequentially** - Don't skip cells
2. **Monitor GPU** - Use `nvidia-smi` in terminal
3. **Save frequently** - Checkpoints every N steps
4. **Visualize early** - Plot metrics during training
5. **Adjust on-the-fly** - Modify hyperparameters between runs

## ğŸ”— Related Files

Make sure you have these from your main project:
- `../tinyphysics.py` - Physics simulator
- `../models/tinyphysics.onnx` - Dynamics model
- `../data/train/` - Training data

## ğŸ“§ Support

Issues with:
- **CUDA errors** - Check CUDA version compatibility
- **Import errors** - Verify all files are created
- **Path errors** - Use absolute paths if needed
