# ===============================
# PPO TRAINING CONFIGURATION
# ===============================

seed: 42

# Environment settings
env:
  data_path: "../../data/test"
  platforms:
    - "CHEVROLET_VOLT_PREMIER_2017"
  num_envs: 8  # Number of parallel environments
  max_steps: 1000  # Max steps per episode
  
# PPO hyperparameters
ppo:
  learning_rate: 3.0e-4
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda
  clip_ratio: 0.2  # PPO clip ratio
  value_coef: 0.5  # Value loss coefficient
  entropy_coef: 0.01  # Entropy bonus coefficient
  max_grad_norm: 0.5  # Gradient clipping
  
  # Training schedule
  num_epochs: 10  # PPO epochs per update
  batch_size: 256  # Mini-batch size
  num_steps: 2048  # Steps before update
  
  # Network architecture
  hidden_sizes: [256, 256]  # Actor-critic hidden layers
  activation: "tanh"  # relu, tanh, elu
  
# Training settings
training:
  total_timesteps: 1000000  # Total training steps
  save_freq: 10000  # Save checkpoint every N steps
  eval_freq: 5000  # Evaluate every N steps
  log_freq: 100  # Log to tensorboard every N steps
  
  # Paths
  model_save_path: "./models"
  log_dir: "./runs"
  
# Evaluation
eval:
  num_episodes: 10
  render: false

# Device
device: "cuda"  # cuda or cpu
