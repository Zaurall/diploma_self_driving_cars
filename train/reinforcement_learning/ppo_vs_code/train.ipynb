{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e49b41a",
   "metadata": {},
   "source": [
    "# PPO Training for Lateral Control\n",
    "\n",
    "This notebook implements **Proximal Policy Optimization (PPO)** for lateral vehicle control.\n",
    "\n",
    "**Optimized for:**\n",
    "- Single GPU (A100) training\n",
    "- JupyterLab environment\n",
    "- No root permissions required\n",
    "- Local training (no AWS/cloud infrastructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48b302b-a2ed-4013-8b4f-bae959ed73d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [i for i in range(10)]\n",
    "a[-2:]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40be0e4",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Dependencies\n",
    "\n",
    "Install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install torch numpy pandas pyyaml tensorboard matplotlib tqdm gym\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import custom modules\n",
    "from environment import LateralControlEnv, VectorizedLateralControlEnv\n",
    "from policy import PPOAgent\n",
    "from utils import RolloutBuffer, Logger\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc3697",
   "metadata": {},
   "source": [
    "## 2. Load Configuration\n",
    "\n",
    "Load training hyperparameters from YAML config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bedf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config['seed'])\n",
    "\n",
    "# Print key configuration\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Seed: {config['seed']}\")\n",
    "print(f\"  Number of environments: {config['env']['num_envs']}\")\n",
    "print(f\"  Learning rate: {config['ppo']['learning_rate']}\")\n",
    "print(f\"  Total timesteps: {config['training']['total_timesteps']}\")\n",
    "print(f\"  Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90641546",
   "metadata": {},
   "source": [
    "## 3. Create Environment\n",
    "\n",
    "Set up vectorized lateral control environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data files\n",
    "data_files = []\n",
    "for platform in config['env']['platforms']:\n",
    "    platform_files = glob.glob(os.path.join(config['env']['data_path'], platform, '*.csv'))\n",
    "    data_files.extend(platform_files)\n",
    "\n",
    "print(f\"Found {len(data_files)} training files\")\n",
    "\n",
    "# Sample files for parallel environments\n",
    "num_envs = config['env']['num_envs']\n",
    "sampled_files = random.sample(data_files, min(num_envs, len(data_files)))\n",
    "\n",
    "# Create vectorized environment\n",
    "model_path = '../models/tinyphysics.onnx'\n",
    "envs = VectorizedLateralControlEnv(\n",
    "    sampled_files,\n",
    "    model_path=model_path,\n",
    "    max_steps=config['env']['max_steps']\n",
    ")\n",
    "\n",
    "print(f\"Created {envs.num_envs} parallel environments\")\n",
    "print(f\"Observation space: {envs.observation_space}\")\n",
    "print(f\"Action space: {envs.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f86a5",
   "metadata": {},
   "source": [
    "## 4. Initialize PPO Agent\n",
    "\n",
    "Create the PPO agent with Actor-Critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a19302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get observation and action dimensions\n",
    "obs_dim = envs.observation_space.shape[0]\n",
    "action_dim = envs.action_space.shape[0]\n",
    "\n",
    "# Create PPO agent\n",
    "agent = PPOAgent(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_sizes=config['ppo']['hidden_sizes'],\n",
    "    activation=config['ppo']['activation'],\n",
    "    learning_rate=config['ppo']['learning_rate'],\n",
    "    gamma=config['ppo']['gamma'],\n",
    "    gae_lambda=config['ppo']['gae_lambda'],\n",
    "    clip_ratio=config['ppo']['clip_ratio'],\n",
    "    value_coef=config['ppo']['value_coef'],\n",
    "    entropy_coef=config['ppo']['entropy_coef'],\n",
    "    max_grad_norm=config['ppo']['max_grad_norm'],\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "print(f\"Created PPO agent:\")\n",
    "print(f\"  Observation dim: {obs_dim}\")\n",
    "print(f\"  Action dim: {action_dim}\")\n",
    "print(f\"  Hidden sizes: {config['ppo']['hidden_sizes']}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in agent.ac.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e1aed",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Main PPO training loop with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(config['training']['model_save_path'], exist_ok=True)\n",
    "os.makedirs(config['training']['log_dir'], exist_ok=True)\n",
    "\n",
    "# Create rollout buffer\n",
    "buffer = RolloutBuffer(\n",
    "    num_envs=num_envs,\n",
    "    num_steps=config['ppo']['num_steps'],\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim\n",
    ")\n",
    "\n",
    "# Training statistics\n",
    "training_stats = {\n",
    "    'episode_rewards': [],\n",
    "    'episode_costs': [],\n",
    "    'episode_lengths': [],\n",
    "    'policy_losses': [],\n",
    "    'value_losses': [],\n",
    "    'entropies': []\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "total_timesteps = config['training']['total_timesteps']\n",
    "num_steps = config['ppo']['num_steps']\n",
    "num_updates = total_timesteps // (num_steps * num_envs)\n",
    "\n",
    "print(f\"Starting PPO training:\")\n",
    "print(f\"  Total updates: {num_updates}\")\n",
    "print(f\"  Total timesteps: {total_timesteps}\")\n",
    "print(f\"  Steps per update: {num_steps}\")\n",
    "print(f\"  Num environments: {num_envs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "global_step = 0\n",
    "\n",
    "for update in tqdm(range(num_updates), desc=\"Training Progress\"):\n",
    "    # Collect rollouts\n",
    "    obs = envs.reset()\n",
    "    episode_rewards_batch = []\n",
    "    episode_costs_batch = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get actions\n",
    "        actions, values = agent.get_action(obs)\n",
    "        \n",
    "        # Compute log probs\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.FloatTensor(obs).to(agent.device)\n",
    "            action_tensor = torch.FloatTensor(actions).to(agent.device)\n",
    "            _, log_probs, _ = agent.ac.evaluate_actions(obs_tensor, action_tensor)\n",
    "            log_probs = log_probs.cpu().numpy().flatten()\n",
    "        \n",
    "        # Step environments\n",
    "        next_obs, rewards, dones, infos = envs.step(actions)\n",
    "        \n",
    "        # Store transition\n",
    "        buffer.add(obs, actions, rewards, values.flatten(), log_probs, dones)\n",
    "        \n",
    "        # Track episodes\n",
    "        for i, (done, info) in enumerate(zip(dones, infos)):\n",
    "            if done:\n",
    "                episode_rewards_batch.append(sum([info.get('cost', 0) for info in infos]))\n",
    "                episode_costs_batch.append(info.get('total_cost', 0))\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    # Compute advantages and returns\n",
    "    with torch.no_grad():\n",
    "        _, next_values = agent.get_action(obs)\n",
    "    \n",
    "    advantages = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "    returns = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "    \n",
    "    for env_idx in range(num_envs):\n",
    "        env_rewards = buffer.rewards[:, env_idx]\n",
    "        env_values = buffer.values[:, env_idx]\n",
    "        env_dones = buffer.dones[:, env_idx]\n",
    "        env_next_value = next_values[env_idx]\n",
    "        \n",
    "        env_advantages, env_returns = agent.compute_gae(\n",
    "            env_rewards, env_values, env_dones, env_next_value\n",
    "        )\n",
    "        \n",
    "        advantages[:, env_idx] = env_advantages\n",
    "        returns[:, env_idx] = env_returns\n",
    "    \n",
    "    # Get rollout data and update\n",
    "    rollout_data = buffer.get(next_values=None, advantages=advantages, returns=returns)\n",
    "    train_stats = agent.update(\n",
    "        rollout_data,\n",
    "        num_epochs=config['ppo']['num_epochs'],\n",
    "        batch_size=config['ppo']['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Update statistics\n",
    "    if episode_rewards_batch:\n",
    "        training_stats['episode_rewards'].append(np.mean(episode_rewards_batch))\n",
    "    if episode_costs_batch:\n",
    "        training_stats['episode_costs'].append(np.mean(episode_costs_batch))\n",
    "    training_stats['policy_losses'].append(train_stats['policy_loss'])\n",
    "    training_stats['value_losses'].append(train_stats['value_loss'])\n",
    "    training_stats['entropies'].append(train_stats['entropy'])\n",
    "    \n",
    "    # Update global step\n",
    "    global_step += num_steps * num_envs\n",
    "    \n",
    "    # Print progress\n",
    "    if update % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Update {update}/{num_updates} | Step {global_step}/{total_timesteps}\")\n",
    "        if training_stats['episode_rewards']:\n",
    "            print(f\"  Mean Reward: {training_stats['episode_rewards'][-1]:.2f}\")\n",
    "        if training_stats['episode_costs']:\n",
    "            print(f\"  Mean Cost: {training_stats['episode_costs'][-1]:.4f}\")\n",
    "        print(f\"  Policy Loss: {train_stats['policy_loss']:.4f}\")\n",
    "        print(f\"  Value Loss: {train_stats['value_loss']:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if update % (config['training']['save_freq'] // (num_steps * num_envs)) == 0:\n",
    "        save_path = os.path.join(\n",
    "            config['training']['model_save_path'],\n",
    "            f\"checkpoint_{global_step}.pt\"\n",
    "        )\n",
    "        agent.save(save_path)\n",
    "    \n",
    "    # Reset buffer\n",
    "    buffer.reset()\n",
    "\n",
    "print(\"\\\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7343fa",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Metrics\n",
    "\n",
    "Plot training curves to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "if training_stats['episode_rewards']:\n",
    "    axs[0, 0].plot(training_stats['episode_rewards'])\n",
    "    axs[0, 0].set_title('Episode Rewards')\n",
    "    axs[0, 0].set_xlabel('Update')\n",
    "    axs[0, 0].set_ylabel('Mean Reward')\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "# Episode costs\n",
    "if training_stats['episode_costs']:\n",
    "    axs[0, 1].plot(training_stats['episode_costs'])\n",
    "    axs[0, 1].set_title('Episode Costs')\n",
    "    axs[0, 1].set_xlabel('Update')\n",
    "    axs[0, 1].set_ylabel('Mean Cost')\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "# Policy loss\n",
    "axs[1, 0].plot(training_stats['policy_losses'])\n",
    "axs[1, 0].set_title('Policy Loss')\n",
    "axs[1, 0].set_xlabel('Update')\n",
    "axs[1, 0].set_ylabel('Loss')\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# Value loss\n",
    "axs[1, 1].plot(training_stats['value_losses'])\n",
    "axs[1, 1].set_title('Value Loss')\n",
    "axs[1, 1].set_xlabel('Update')\n",
    "axs[1, 1].set_ylabel('Loss')\n",
    "axs[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['training']['model_save_path'], 'training_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8d238",
   "metadata": {},
   "source": [
    "## 7. Save Model\n",
    "\n",
    "Save the final trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(config['training']['model_save_path'], \"final_model.pt\")\n",
    "agent.save(final_model_path)\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "\n",
    "# Save training statistics\n",
    "stats_path = os.path.join(config['training']['model_save_path'], \"training_stats.npz\")\n",
    "np.savez(stats_path, **training_stats)\n",
    "print(f\"Training stats saved to: {stats_path}\")\n",
    "\n",
    "# Close environments\n",
    "envs.close()\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
